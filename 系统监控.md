# 系统监控

参考资料:  
[Elastic Stack 中文指南](https://elkguide.elasticsearch.cn/)  
[Running the Elastic Stack on Docker](https://www.elastic.co/guide/en/elastic-stack-get-started/7.5/get-started-docker.html)

架构技术选型：

+ **日志监控 ELK**

  + ++日志收集 Scribe、Fluent、Flume、Logstash、Rsyslog、Scripts++
    
    + Fluent
    + Logstash
    
  + ++日志汇总 Kafka++

    > 1、分布式架构，可支持水平扩展。  
    > 2、高吞吐量，在普通的服务器上每秒钟也能处理几十万条消息。  
    > 3、消息持久化，按topic分区存储，支持可重复消费。  
    > 4、日志系统不需要保证严格的消息准确性。  
    > 5、数据在磁盘上的存取代价为O(1)。  
    > 6、可根据broker配置定期删除过期数据。

  + ++日志处理 Logstash++

    功能包括：数据采集、输入、过滤、输出。
    适用于各种形式和大小的数据。   

    暂时研究的问题：  
    1) 如何创建pipeline，以Apache日志作为输入，将日志解析成结构化数据并存储到ES集群？
    2) 如何结合多个输入输出插件实现对不同来源的数据的合并？
    
    > Metrics 指的是什么？  
    > event ?  
    > pipeline 应该怎么理解？

  + ++日志存储 Elasticsearch++

  + ++日志数据可视化 Kibana++

+ **系统资源监控 cAdvisor+Prometheus+influxdb+grafana**
  
  参考：[uschtwill/docker_monitoring_logging_alerting](https://github.com/uschtwill/docker_monitoring_logging_alerting)

  + ++容器监控 cAdvisor++

    cadvisor不仅可以搜集一台机器上所有运行的容器信息，还提供基础查询界面和http接口，方便其他组件如Prometheus进行数据抓取

  + ++Prometheus++

    Prometheus 是由 SoundCloud 开源监控告警解决方案；存储的是时序数据，即按相同时序(相同名称和标签)，以时间维度存储连续的数据的集合。

    - AlertManager

      Prometheus自带的报警组件。

  + ++influxdb++

    时序数据库

  + ++Grafana++

+ **监控报警**
  
  + ++Zabbix++

参考架构：
![](https://afoo.me/columns/tec/images/logging-platform.png)


服务1/2/3/... -> 日志采集工具 -> kafka/rocketmq -> logstash -> Elasticsearch -> Kibana

## Filebeat

Filebeat是一个独立的服务，用于从日志文件提取日志信息，发送给ES、Logstash、Kafka、Redis等。

如果要在集群中使用Filebeat作为日志采集器，同样要下载、安装、配置、启动其服务。

[Configuring Filebeat to Send Log Lines to Logstash](https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html#configuring-filebeat)

[Configuring Logstash for Filebeat Input](https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html#_configuring_logstash_for_filebeat_input)

  ![Filebeat工作原理简图](https://www.elastic.co/guide/en/beats/filebeat/current/images/filebeat.png)

## Logstash

[Logstash Refernce](https://www.elastic.co/guide/en/logstash/current/index.html)  
[Logstash中文指南](https://elkguide.elasticsearch.cn/logstash/) （入门推荐，官网讲的太过详细反而不太适合快速入门）

### 基础入门

#### 最简测试

```shell
#从标准输入获取输入数据，不经filter直接输出到标准输出，输出时使用rubydebug编码器编码
bin/logstash -e 'input{stdin{}}output{stdout{codec=>rubydebug}}'
```
比如命令行输入“lee”，结果打印
```json
{
    "@version" => "1",
    "message" => "lee",
    "host" => "Lee-Home",
    "@timestamp" => 2020-01-30T03:34:14.830Z
}
```
`@version`,`host`,`@timestamp`（事件发生时间）是logstash添加的额外的信息。

#### 基本概念：

+ 事件

  可以粗略理解为输入到logstash的数据。

#### 接口信息

需要理清API接口有哪些，通信端口有哪些？

##### 端口配置

##### APIs


#### DSL语法

[Logstash DSL 语法](https://elkguide.elasticsearch.cn/logstash/get-start/full-config.html)

### 工作原理

#### 四个组件

包含三个阶段：输入->过滤->输出。

**输入器（可以通过input-plugins拓展）**：
file、syslog、redis、beats ...   
[input-plugins](https://www.elastic.co/guide/en/logstash/current/input-plugins.html)  
本来打算使用logback打日志的，但是官方没有提供对应的input插件。TODO：?

+ kafka

**过滤器（不单单是过滤的功能,可以组合使用，可通过filter-plugins拓展）**：
grok、mutate、drop、clone、geoip ...  
[filter-plugins](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html)

+ grok（logstash-filter-grok）

  将非结构化事件数据解析为字段

+ json

**输出器（将处理后的数据输出到指定的存储，可通过output-plugins拓展）**：
elasticsearch、file、graphite、statsd ...  
[output-plugins](https://www.elastic.co/guide/en/logstash/current/output-plugins.html)

+ elasticsearch

+ email

+ influxdb

+ zabbix

**编解码器（在input之前和output之后起作用，可通过codec-plugins拓展）**:  
json、multiline ...  
[codec-plugins](https://www.elastic.co/guide/en/logstash/current/codec-plugins.html)

+ json

#### 执行模型

没有找到详细的图，下面这个图相对还行。  
但是需要补充一下：图中filter可不是一个，而是一个filter链。
![](https://img-blog.csdn.net/20180714153238934?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzM1OTMwMjU5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Logstash事件处理管道

队列事件持久化（防止事件丢失）
[Persistent Queues](https://www.elastic.co/guide/en/logstash/current/persistent-queues.html)

### 安装运行

#### 配置

##### 设置文件

+ [logstash.yml](https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html#logstash-settings-file)

+ [pipeline.yml](https://www.elastic.co/guide/en/logstash/current/multiple-pipelines.html)

+ jvm.options

+ log4j2.properties

+ startup.options

##### 管道配置文件

Pipeline配置文件位置：docker容器`/usr/share/logstash/pipeline/`。

#### Docker容器运行（单机）

```
#测试启动
docker run --rm -it logstash:7.5.1
#生产部署（把需要配置的文件映射到本地）
docker run -di 
  --name logstash-single
  -v /home/lee/docker/logstash/log:/var/log/logstash
  logstash:7.5.1
```




### 整合多输入输出


### 数据安全

+ 队列数据持久化

+ 死信队列

### 自身状态监控

+ 监控API

+ X-Pack

### 性能优化


### 模块/插件集成

+ X-Pack（默认安装）

  Elastic stack 的拓展，提供安全、报警、监控、机器学习、pipeline管理等功能。
  是默认安装的。


## ElasticSearch

两个端口：  
9200: ES与外部通信的端口；  
9300: ES节点之间通信的端口。

ES索引类型文档与DB对应关系：

| ElasticSearch | 关系型数据库 | 说明 |
| :-----------: | :---:| :---: |
| Index | Database | 索引名称 |
| TYPE | Table | 一个索引库只能有一个表 |
| Document | Row |
| Field | Column | 分为StringField和TextField |
| Mapping | Schema | Schema约束：字段长度、类型等；Mapping约束：字段类型、是否分词、是否存储、是否索引等 |

### 本地启动，文档存储与索引创建、搜索、聚合分析

下载软件包解压，然后执行`bin/elasticsearch`,如果要创建集群的话，
则需要额外指定数据目录和日志目录。
```
bin/elasticsearch -Epath.data=data2 -Epath.logs=log2
```
docker创建集群
```

```
查看ES集群状态。
```
GET /_cat/health?v
```
文件存储与索引创建（支持批量操作）
```shell
PUT /customer/_doc/1
{
  "name": "John Doe"  #"name"是索引库的field字段
}
GET /customer/_doc/1
GET /bank/_search

```
### ES配置

#### JVM配置

+ 堆空间

#### 安全配置

#### log4j2日志配置

#### X-Pack配置

X-Pack 是一个 Elastic Stack 拓展，提供安全、报警、监控、上报、机器学习以及很多其他的功能，注意X-Pack是针对ELK集群执行监控等操作。

### ES聚合操作

### Query DSL

ES自定义的一种查询语言，其实是一种查询规范。

叶子查询子句
复合查询子句

### ES UI页面（Elasticsearch-head）

```
docker pull mobz/elasticsearch-head:5-alpine

docker run -di --name es-head-single -p 9100:9100 mobz/elasticsearch-head:5-alpine
```

### 索引存储机制

感觉和MySQL的存储机制很像。


## Kibana

[Kibana 中文用户手册](https://www.elastic.co/guide/cn/kibana/current/introduction.html)  
但是需要注意中文手册对应版本比较老旧，但是适合快速浏览。

### 环境搭建

[Kibana配置项](https://www.elastic.co/guide/cn/kibana/current/settings.html)

[环境变量与配置文件中配置项对应表](https://www.elastic.co/guide/cn/kibana/current/docker.html#docker-env-config)  
“.”转为“_”，全部转为大写。

访问入口`localhost:5601`;  
状态检查`localhost:5601/status`

索引模式：匹配索引名称；  
使用默认的 logstash-* 作为索引模式可以索引 logstash 送进ES后建立的索引。  


### 基础入门

### 数据探索

### 可视化

### 仪表盘

### 时序控件

### 控制台

### 管理

### 插件


